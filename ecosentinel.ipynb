{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EcoSentinel #\n",
        "Welcome to the EcoSentinel pipeline. In this notebook, you can find all the code necessary to run the EcoSentinel pipeline on your own hardware, unlocking \n",
        "the same insights we have worked hard to develop. We are making this project completely open-source in an effort to stay transparent about our methods of analysis, hopefully earning some trust from you on the information we provide.\n",
        "\n",
        "Navigate through each subsection in the following notebook to run your own analysis on a country of your choosing.\n",
        "\n",
        "Good luck and happy analyzing!\n",
        "\n",
        "Regards,\n",
        "\n",
        "Arjun, Arnav, Zhaolin\n",
        "\n",
        "*(The EcoSentinel Team)*\n"
      ],
      "metadata": {
        "id": "Me0KEIVO9KrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Earth Engine & TensorFlow Integration ##\n",
        "The first step in the pipeline is Google Earth Engine, feeding us valuable real-time and historical land-satellite imagery that allows us to train our model and analyze images as they are collected. By using the EE API, we can pipe our images from the Earth Engine into our TensorFlow model for training and analyzing.\n",
        "\n",
        "The training for our model has already been done, so the calibration for the model in this notebook is preloaded. You are then able to pipe a specific Earth Engine dataset into the model for analysis if wanted, but for now we will leave the majority of the training functionality and model deployment out of this notebook to simplify the process.\n",
        "\n",
        "Note: authentication will be required if you choose to run this part of the pipeline **in the future** with Earth Engine."
      ],
      "metadata": {
        "id": "oqIwObzZfRFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Regressive Analysis ##\n",
        "A novel algorithm we've developed, the multi-regressive analysis allows us to produce the best linear outlook for a dataset dependent on time as one of the many features.\n",
        "\n",
        "By combining long-term and short-term linear trends, we can produce a weighted forecast for a given period of time. This, in combination with a normal regressive model that figures out the association between every feature we consider and deforestation, allows us to forecast deforestation trends in the future.\n",
        "\n",
        "These forecasts are then pushed into our custom derived formulas for risk and grading.\n",
        "\n",
        "If that sounds a bit complicated, just think of it as finding trends in the present and past to forecast future trends. To start, adjust the parameters given or run as is to get a good look as to what we're doing to analyze\n",
        "\n",
        "### Multi-Regressive Algorithm ###"
      ],
      "metadata": {
        "id": "Jj-BZFUvh-62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "== RUN THIS CELL ==\n",
        "defines necessary methods for running the program\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "    Data import & engineering functions.\n",
        "\"\"\"\n",
        "import matplotlib as plt            # plotting for visualization\n",
        "import numpy as np                  # data manipulation\n",
        "import pandas as pd                 # data reading/writing\n",
        "from sklearn import linear_model    # linear modeling\n",
        "from math import floor              # time-frame calculation\n",
        "\n",
        "# full data process #\n",
        "def data_import(dir):\n",
        "    # load datasets #\n",
        "    data, cols = read_data(dir)\n",
        "\n",
        "    # disregard time #\n",
        "    cols.remove(\"time\")\n",
        "    col_indxs = [ cols.index(col) for col in cols ]\n",
        "\n",
        "    data = data[ : , col_indxs ]\n",
        "\n",
        "    # split data #\n",
        "    train_data, train_output = split_io(data)\n",
        "\n",
        "    # normalize input #\n",
        "    train_data = normalize(train_data)\n",
        "\n",
        "    # return datasets #\n",
        "    return [ cols, train_data, train_output ]\n",
        "\n",
        "\n",
        "# full data process #\n",
        "def feature_import(dir, feature_name):\n",
        "    # load datasets #\n",
        "    data, cols = read_data(dir)\n",
        "\n",
        "    # focus data #\n",
        "    time_indx = cols.index(\"time\")\n",
        "    feature_indx = cols.index(feature_name)\n",
        "    data = data[ : , [time_indx, feature_indx] ]\n",
        "\n",
        "    # sort data #\n",
        "    data = np.flip( data[ data[:, 0].argsort() ], axis=0)\n",
        "\n",
        "    # split data #\n",
        "    time_data, feature_data = split_io(data)\n",
        "\n",
        "    # normalize input #\n",
        "    time_data = normalize(time_data)\n",
        "\n",
        "    # find current info #\n",
        "    cur_date = time_data[0][0]\n",
        "    cur_val = feature_data[0]\n",
        "\n",
        "    # return datasets #\n",
        "    return [ [\"date\", feature_name], time_data, feature_data, (cur_date, cur_val) ]\n",
        "\n",
        "\n",
        "# exports data #\n",
        "def data_export(dataset, col_labels, output_dir):\n",
        "    # dataframe creation #\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # deploy data #\n",
        "    df.to_csv(output_dir, columns=col_labels)\n",
        "\n",
        "\n",
        "# reads dataset #\n",
        "def read_data(dir):\n",
        "    # read into panda dataframe #\n",
        "    data = pd.read_csv(dir, delimiter=',')\n",
        "\n",
        "    # convert to numpy array #\n",
        "    data_arr = np.array(data.values, \"float\")\n",
        "    data_cols = data.columns.to_list()\n",
        "\n",
        "    # give back info #\n",
        "    return data_arr, data_cols\n",
        "\n",
        "\n",
        "# ensure data format #\n",
        "def format_data(data):\n",
        "    # # check dimensions #\n",
        "    # if np.atleast_2d(data).shape[0] != 1:\n",
        "    #     return data\n",
        "    \n",
        "    # check transposing #\n",
        "    if np.atleast_2d(data).shape[0] < np.atleast_2d(data).shape[1]:\n",
        "        data = np.atleast_2d(data).T\n",
        "\n",
        "    # default #\n",
        "    return data\n",
        "\n",
        "\n",
        "# normalize data #\n",
        "def normalize(input):\n",
        "    # normalize inputs #\n",
        "    # input = (input - input.min(axis=0)) * 100 / (input.max(axis=0) - input.min(axis=0))\n",
        "    input = (input - input.min(axis=0))\n",
        "\n",
        "    # return split data #\n",
        "    return input\n",
        "\n",
        "\n",
        "# split data into input & output #\n",
        "def split_io(data):\n",
        "    # dimensions #\n",
        "    num_elements, num_features = data.shape\n",
        "    num_features -= 1\n",
        "\n",
        "    # split data #\n",
        "    input = np.atleast_2d(data)[ : , : num_features]\n",
        "    expected_output = np.atleast_2d(data)[ :, num_features]\n",
        "\n",
        "    # return split #\n",
        "    return input, expected_output\n",
        "\n",
        "\n",
        "# splits into training, test, and cross-validation sets #\n",
        "def split_data(data):\n",
        "    # dimensions #\n",
        "    num_elements, num_features = np.atleast_2d(data).shape\n",
        "\n",
        "    # percent split #\n",
        "    train_entries = int(num_elements * .7)\n",
        "    test_entries = int(num_elements * .15)\n",
        "    cv_entries = num_elements - (train_entries + test_entries)\n",
        "\n",
        "    test_start = train_entries + 1\n",
        "    cv_start = train_entries + test_entries + 1\n",
        "\n",
        "    # randomize #\n",
        "    np.random.shuffle(data)\n",
        "\n",
        "    # split #\n",
        "    train_data = np.atleast_2d(data)[ : test_start, : ]\n",
        "    test_data = np.atleast_2d(data)[test_start : cv_start, : ]\n",
        "    cv_data = np.atleast_2d(data)[cv_start : , : ]\n",
        "\n",
        "    # return #\n",
        "    return train_data, test_data, cv_data\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Self-Referential feature evolution model.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# set params #\n",
        "global weight_distribution\n",
        "global time_frame_ratio\n",
        "global cur_val\n",
        "global cur_date\n",
        "global pred_range\n",
        "\n",
        "\n",
        "# conduct algorithm #\n",
        "def self_contained_regression(dir, feature_name, weights, time_ratio, pred_yrs):\n",
        "    # params #\n",
        "    set_params(weights, time_ratio, pred_yrs)\n",
        "    regr_looks = []\n",
        "\n",
        "    # import #\n",
        "    feature_data = feature_import(dir, feature_name)\n",
        "\n",
        "    col_labels = feature_data[0]\n",
        "    data = feature_data[1]\n",
        "    feature_output = format_data(feature_data[2])\n",
        "    \n",
        "    global cur_date\n",
        "    cur_date = feature_data[3][0]\n",
        "    global cur_val\n",
        "    cur_val = feature_data[3][1]\n",
        "\n",
        "    # time frames #\n",
        "    time_frames = split_time_frame(time_data=data, frame_ratio=time_frame_ratio)\n",
        "    \n",
        "    # train models #\n",
        "    regr_looks = train_regr_looks(time_frames=time_frames, input=data, output=feature_output, fix=False)\n",
        "\n",
        "    # multi-regressive model #\n",
        "    self_regr = regr_weighted(regr_looks=regr_looks, weight_distribution=weight_distribution)\n",
        "    \n",
        "    # predictions #\n",
        "    regr_preds = regr_prediction(regr_looks=regr_looks, input=data, time_frames=time_frames)\n",
        "\n",
        "    self_pred_data = np.arange(cur_date, int(cur_date + pred_range)).reshape(-1, 1)\n",
        "    self_pred = self_regr.predict(self_pred_data)\n",
        "\n",
        "    # visualize #\n",
        "    plot_feature_looks(regr_preds=regr_preds, self_pred=self_pred, input=data, output=feature_output, pred_data=self_pred_data,\\\n",
        "                       time_frames=time_frames, col_labels=col_labels)\n",
        "    \n",
        "    # return outputs #\n",
        "    return [ self_regr, self_pred ]\n",
        "\n",
        "\n",
        "# weighted distribution of regressive looks #\n",
        "def regr_weighted(regr_looks, weight_distribution):\n",
        "    # apply weighting #\n",
        "    self_coef = []\n",
        "    sum_weights = np.sum( np.array(weight_distribution) )\n",
        "\n",
        "    for look_num in range(len(regr_looks)):\n",
        "        self_coef.append( weight_distribution[look_num] * regr_looks[look_num][0] )\n",
        "    \n",
        "    # averaging #\n",
        "    self_coef = np.array([ np.asarray(self_coef).mean(axis=0) ]) / sum_weights\n",
        "\n",
        "    # return model #\n",
        "    self_regr = linear_model.LinearRegression()\n",
        "    self_regr.coef_ = self_coef\n",
        "    self_regr.intercept_ = cur_val\n",
        "\n",
        "    return self_regr\n",
        "\n",
        "\n",
        "# conducts predictions for all models #\n",
        "def regr_prediction(regr_looks, input, time_frames):\n",
        "    # make prediction #\n",
        "    regr_preds = []\n",
        "\n",
        "    for look_num in range(len(time_frames)):\n",
        "        regr_look_info = regr_looks[look_num]\n",
        "        regr_look = linear_model.LinearRegression()\n",
        "\n",
        "        regr_look.coef_ = np.array([regr_look_info[0]])\n",
        "        regr_look.intercept_ = np.array([regr_look_info[1]])\n",
        "\n",
        "        regr_preds.append(regr_look.predict(input[ : time_frames[look_num]]))\n",
        "\n",
        "    # return regressive predictions #\n",
        "    return regr_preds\n",
        "\n",
        "\n",
        "# train regressive looks #\n",
        "def train_regr_looks(time_frames, input, output, fix):\n",
        "    \"\"\"\n",
        "        add trained model for each regressive look:\n",
        "              regr_look[i]      = ith regressive output\n",
        "              regr_look[i][0]   = ith regressive output's coefficient coefficient\n",
        "              regr_look[i][1]   = ith regressive output's intercept\n",
        "    \"\"\"\n",
        "    regr_looks = []\n",
        "    \n",
        "    for frame in time_frames:\n",
        "        model_info = optimize( input[ : frame], output[ : frame], cur_val=output[frame - 1][0], fix_intercept=fix )\n",
        "        regr_looks.append([ model_info[1][0], model_info[2] ])\n",
        "    \n",
        "    return np.array(regr_looks)\n",
        "\n",
        "\n",
        "# create custom weighting based on predictive range #\n",
        "def distribute_weights(pred_range, skew, num_timeframes):\n",
        "    \"\"\"\n",
        "        This will eventually replace the need to pass in weights and timeframe ratios.\n",
        "        pred_range: number of time units to predict to\n",
        "        skew:       -1 is left (longer term) skew,\n",
        "                    0 is normal curve,\n",
        "                    1 is right (shorter term skew)\n",
        "    \"\"\"\n",
        "    # to be implemented #\n",
        "    return weight_distribution\n",
        "\n",
        "\n",
        "# divide data into time frames #\n",
        "def split_time_frame(time_data, frame_ratio):\n",
        "    # range #\n",
        "        # begin = np.min(time_data)\n",
        "        # end = np.max(time_data)\n",
        "        # range = end - begin\n",
        "    range = len(time_data)\n",
        "\n",
        "    # divide #\n",
        "    return [ floor(ratio * range) for ratio in frame_ratio ]\n",
        "\n",
        "\n",
        "# sets global parameters #\n",
        "def set_params(weights, time_ratio, pred_yrs):\n",
        "    \"\"\"\n",
        "        weights:    distribution of weights along each time frame\n",
        "        time ratio: ratio of time units for each time frame\n",
        "        pred_yrs:   number of years to forecast to\n",
        "    \"\"\"\n",
        "    global weight_distribution\n",
        "    weight_distribution = weights\n",
        "\n",
        "    global time_frame_ratio\n",
        "    time_frame_ratio = time_ratio\n",
        "\n",
        "    global pred_range\n",
        "    pred_range = pred_yrs * 365\n",
        "\n",
        "\n",
        "# divide data into time frames #\n",
        "def split_time_frame(time_data, frame_ratio):\n",
        "    # range #\n",
        "    begin = np.min(time_data)\n",
        "    end = np.max(time_data)\n",
        "    range = end - begin\n",
        "\n",
        "    # divide #\n",
        "    return [ratio * range for ratio in frame_ratio]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Regression calculation.\n",
        "\"\"\"\n",
        "# returns optimized linear model #\n",
        "def optimize(input, exp_out, cur_val, fix_intercept):\n",
        "    # model creation #\n",
        "    regr_look = linear_model.LinearRegression(fit_intercept=(not fix_intercept))\n",
        "    if fix_intercept:\n",
        "        exp_out = exp_out - cur_val\n",
        "    \n",
        "    # train #\n",
        "    regr_look.fit(input, exp_out)\n",
        "\n",
        "    # return model & parameters #\n",
        "    return [ regr_look, regr_look.coef_[0], cur_val if fix_intercept else regr_look.intercept_[0] ]\n",
        "\n",
        "\"\"\"\n",
        "    Visualization functions.\n",
        "\"\"\"\n",
        "# plots all regressive looks #\n",
        "def plot_forecasts(data, exp_out, future_data, pred_data):\n",
        "    # plot data #\n",
        "    plt.scatter(data, exp_out, color=\"black\")\n",
        "\n",
        "    # plot forecasts #\n",
        "    plt.plot(future_data, pred_data, color=\"forestgreen\", label=\"Multi-Regressive Forecast\")\n",
        "\n",
        "    # labels #\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"features\")\n",
        "    plt.ylabel(\"model output\")\n",
        "\n",
        "    # show plot #\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# plots regressive looks for one feature #\n",
        "def plot_feature_looks(regr_preds, self_pred, input, output, pred_data, time_frames, col_labels):\n",
        "    # plot data #\n",
        "    plt.plot(input, output, color=\"black\")\n",
        "    num_frames = len(time_frames)\n",
        "\n",
        "    # draw each regressive look #\n",
        "    color = iter(plt.cm.rainbow(np.linspace(0, 1, num_frames + 1)))\n",
        "    for f in range(num_frames):\n",
        "        c = next(color)\n",
        "        plt.plot(input[ : time_frames[f] ], regr_preds[f], color=c, label=f\"Regressive Look {f}\")\n",
        "\n",
        "    # draw self predictive look #\n",
        "    c = next(color)\n",
        "    plt.plot(pred_data, self_pred, color=c, label=f\"Self-Predictive Look\")\n",
        "\n",
        "    # labels #\n",
        "    plt.legend()\n",
        "    plt.xlabel(col_labels[0])\n",
        "    plt.ylabel(col_labels[1])\n",
        "\n",
        "    # show plot #\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "SFFLm7nhQJiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "== SET PARAMETERS & RUN THIS CELL ==\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "    This algorithm takes multiple regressive looks at the same set of data [with respect to different timeframes] \n",
        "    to generate the most accurate linear outlook for a given time period.\n",
        "    \n",
        "    This works by taking multiple \"regressive looks\" at the same dataset for long- and short-term trends and then \n",
        "    weighing the linear models using a skewed distribution to predict for a given future time period.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "    Multi-Regressive model, combining all feature forecasts to predict future deforestation.\n",
        "\n",
        "    ======= Set Params for Testing ========\n",
        "    dir             = \"\"        | string for the directory to source data from\n",
        "    feature_set     = [\"\"]      | features to predict and consider; num_features inferred from here; time assumed always a feature\n",
        "    weight_dists    = [[#.#]]   | weight distributions for each feature in feature_set\n",
        "    time_ratios     = [[#.#]]   | time frame ratios for each weight distribution in weight_dists    \n",
        "    pred_yrs        = #.#       | number of years to predict to (will output predictions for every day till then)\n",
        "\"\"\"\n",
        "global dir\n",
        "dir = \"\"                                        # ADJUST THIS PARAMETER IF WANTED\n",
        "\n",
        "global feature_set              \n",
        "feature_set = [\"\"]                              # ADJUST THIS PARAMETER IF WANTED\n",
        "global final_feature                \n",
        "final_feature = \"\"                              # ADJUST THIS PARAMETER IF WANTED\n",
        "\n",
        "global weight_dists\n",
        "weight_dists = [[25, 30, 25, 20, 15, 5, 1]]     # ADJUST THIS PARAMETER IF WANTED\n",
        "global time_ratios\n",
        "time_ratios = [[1, .75, .5, .25, .1, .05, .01]] # ADJUST THIS PARAMETER IF WANTED\n",
        "\n",
        "global pred_yrs\n",
        "pred_yrs = 3                                    # NOT TO BE ADJUSTED (CALIBRATED FOR PERFORMANCE)\n",
        "\n",
        "\n",
        "# conduct algorithm #\n",
        "def multi_regression():\n",
        "    # check params #\n",
        "    check_params()\n",
        "\n",
        "    # declare vars #\n",
        "    self_contained_models = []      # stores final models for each feature\n",
        "    self_contained_forecasts = []   # stored final forecasts (for the given predictive range) for each feature\n",
        "\n",
        "    # import #\n",
        "    dataset = data_import(dir)\n",
        "    col_labels = dataset[0]\n",
        "\n",
        "    data = dataset[1]\n",
        "    output = format_data(dataset[2])\n",
        "    \n",
        "    # build associations for each feature #\n",
        "    multi_regr_model = optimize(input=data, exp_out=output, cur_val=-1, fix_intercept=False)\n",
        "    multi_regr = multi_regr_model[0]\n",
        "\n",
        "    # train self-referencing feature predictions #\n",
        "    for feat_num in range(len(feature_set)):\n",
        "        self_contained_out = self_contained_regression(dir,\n",
        "                                                       feature_name=feature_set[feat_num],\n",
        "                                                       weights=weight_dists[feat_num],\n",
        "                                                       time_ratio=time_ratios[feat_num],\n",
        "                                                       pred_yrs=pred_yrs\n",
        "                                                      )\n",
        "        self_contained_models.append(self_contained_out[0])\n",
        "        self_contained_forecasts.append(self_contained_out[1])\n",
        "    \n",
        "    # predictions #\n",
        "    forecasted_feature_data = produce_forecasted_data(forecasts=self_contained_forecasts)\n",
        "    forecast_predictions = multi_regr.predict(forecasted_feature_data)\n",
        "\n",
        "    # visualize #\n",
        "    plot_forecasts(data=data, exp_out=output, future_data=forecasted_feature_data, pred_data=forecast_predictions)\n",
        "\n",
        "\n",
        "# produces the final predictive dataset for all features #\n",
        "def produce_forecasted_data(forecasts):\n",
        "    # combine all forecasts #\n",
        "    forecasted_data = np.stack(forecasts, axis=1)\n",
        "    return forecasted_data\n",
        "\n",
        "\n",
        "# checks params are correctly inputted #\n",
        "def check_params():\n",
        "    num_features = len(feature_set)\n",
        "\n",
        "    if num_features <= 0:\n",
        "        print(\"ERROR: positive integer number of features required\")\n",
        "        quit()\n",
        "    \n",
        "    if len(weight_dists) != num_features:\n",
        "        print(\"ERROR: check matching length for weight_dists and feature_set\")\n",
        "        quit()\n",
        "\n",
        "    if len(time_ratios) != num_features:\n",
        "        print(\"ERROR: check matching length for time_ratios and feature_set\")\n",
        "        quit()\n",
        "\n",
        "    if len(time_ratios[0]) != len(weight_dists[0]):\n",
        "        print(\"ERROR: check matching length for weight_dists[0] and time_ratios[0]\")\n",
        "        quit()"
      ],
      "metadata": {
        "id": "xAdMpSNdRDcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Risk Analysis ###\n",
        "While we are in the process of reaching out to Environmental Science professors to aid in a formula for gauging the risk of a country's ecosystems of degenerating past a point of *natural* recovery, we are confident in the custom derived formula we have built for the time being.\n",
        "\n",
        "We have created a formula that models behavior similar to what we as developers expect in terms of risk. Note the comments that explain further within the program."
      ],
      "metadata": {
        "id": "Vub8snwgSDpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    This algorithm takes the results of the multi-regressive time series analysis to assign a grade\n",
        "    to countries based on their efforts in protecting the natural landscapes within their borders.\n",
        "\n",
        "    This algorithm only considers one country at a time. To run against multiple countries, you will \n",
        "    have to call it against each one in a separate script/program.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "    Define variables & constants used throughout the program\n",
        "\"\"\"\n",
        "dir = \"\"                                                                # directory for input data for risks over times\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Load data, conduct analysis, and export results\n",
        "\"\"\"\n",
        "def grade_calc():\n",
        "    # gauge risk trends #\n",
        "    risk_trend = self_contained_regression(dir=dir,\n",
        "                                           feature_name=\"risks\",\n",
        "                                           weights=[5, 10, 20, 40, 20, 15], \n",
        "                                           time_ratio=[1, .5, .25, .125, .0625, 0.03125],\n",
        "                                           pred_yrs=3)\n",
        "    \n",
        "    risk_prediction = risk_trend[1][-1]     # get last predicted risk value\n",
        "\n",
        "    # gauge legislative trends #\n",
        "    legislative_trend = calc_legislative_trend()\n",
        "\n",
        "    # calculate grade #\n",
        "    return calc_grade(risk=risk_prediction, leg_effort=legislative_trend)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Grade calculations\n",
        "\"\"\"\n",
        "def calc_grade(risk, leg_effort):\n",
        "    # define grade parameters #\n",
        "    w_leg = 25.0                # weighting constant for legislative efforts in the grade calculation\n",
        "    w_risk = -50.0              # weighting constant for risk trends in the grade calculation\n",
        "    grade_offset = 75.0         # default grade given perfectly neutral parameters\n",
        "    max_risk = 200.0            # max risk value that can be assigned\n",
        "\n",
        "    # calculate risk #\n",
        "    grade = grade_offset + w_leg * leg_effort + w_risk * ( float(risk) / max_risk )\n",
        "    final_grade = match_grade(grade=grade)\n",
        "        \n",
        "    # output grade #\n",
        "    return final_grade\n",
        "\n",
        "\n",
        "def match_grade(grade):\n",
        "    if grade >= 97:\n",
        "        return \"A+\"\n",
        "    elif grade >= 93:\n",
        "        return \"A\"\n",
        "    elif grade >= 90:\n",
        "        return \"A-\"\n",
        "    elif grade >= 87:\n",
        "        return \"B+\"\n",
        "    elif grade >= 83:\n",
        "        return \"B\"\n",
        "    elif grade >= 80:\n",
        "        return \"B-\"\n",
        "    elif grade >= 77:\n",
        "        return \"C+\"\n",
        "    elif grade >= 73:\n",
        "        return \"C\"\n",
        "    elif grade >= 70:\n",
        "        return \"C-\"\n",
        "    elif grade >= 67:\n",
        "        return \"D+\"\n",
        "    elif grade >= 63:\n",
        "        return \"D\"\n",
        "    elif grade >= 60:\n",
        "        return \"D-\"\n",
        "    return \"F\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    The legislative analysis will be implemented in the future, we are just laying out the \n",
        "    framework right now for future expansion.\n",
        "\n",
        "    Read the comment in legislation_analysis.py for further info\n",
        "\"\"\"\n",
        "def calc_legislative_trend():\n",
        "    return 0"
      ],
      "metadata": {
        "id": "eriHbuVcSIZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Grade Derivation ###\n",
        "While we are in the process of reaching out to Environmental Science professors to aid in a formula for grading countries on their efforts, we have in the mean time derived a custom formula that models the behavior we expect from our grading system.\n",
        "\n",
        "As you can see, we reward and punish countries for both their legislative efforts and their efficacy in managing the risk of their ecosystems degenerating beyond a point of recovery."
      ],
      "metadata": {
        "id": "TiWLZFSjTao6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    This algorithm takes the results of the multi-regressive time series analysis to calculate future risk of\n",
        "    a specific ecosystem being damaged beyond the point of recovery.\n",
        "\n",
        "    The assumption being made is that all features will have been previously normalized & flipped so that \n",
        "    positive 1 is always high correlation with deforestation (for each feature) and negative one is always \n",
        "    a high corrrelation with reforestation. This assumption will be enforced in the program.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np                  # arrays and data manipulation \n",
        "import pandas as pd                 # working with data i/o\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Define variables & constants used throughout the program\n",
        "\"\"\"\n",
        "dir = \"\"                                                                # directory for input data\n",
        "cols = [\"countries\", \"proj_forest_cover\", \"min_recorded_cover\"]         # dataset labels\n",
        "output_dir = \"\"                                                         # directory for output\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Load data, conduct analysis, and export data\n",
        "\"\"\"\n",
        "def risk():\n",
        "    # load data #\n",
        "    data = pd.read_csv(dir, ',', usecols=cols)\n",
        "    countries = data[0]\n",
        "    proj_for_covs = data[1]\n",
        "    min_recorded_covs = data[2]\n",
        "\n",
        "    # calculate risk #\n",
        "    risk_analysis = calc_risk(proj_forest_covers=proj_for_covs, min_forest_covers=min_recorded_covs)\n",
        "\n",
        "    # create data frame #\n",
        "    risk_df = np.vstack(countries, risk_analysis).T\n",
        "    risk_df = pd.DataFrame(risk_df)\n",
        "\n",
        "    # deploy data #\n",
        "    risk_df.to_csv(output_dir, columns=[\"countries\", \"projected_risk\"])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    Risk analysis calculations\n",
        "\n",
        "    Risk will be highly dependent on long- and short-term trends, so we need a way of taking the final result\n",
        "    from the multi-regressor to predict future growth and then calculate a range of error and final risk.\n",
        "\n",
        "    Risk will be normalized prior to the final calculation by standardizing the regressive coefficients.\n",
        "\"\"\"\n",
        "def calc_risk(proj_forest_covers, min_forest_covers):\n",
        "    # define risk #\n",
        "    alpha_neg = 1.0             # proportional constant in case risk needs to be shifted\n",
        "    alpha_pos = 1.0             # proportional constant in case risk needs to be shifted\n",
        "    risks = []                  # risks for each country specified\n",
        "\n",
        "    # calculate risk #\n",
        "    for proj_cov, min_cov in zip(proj_forest_covers, min_forest_covers):\n",
        "        if min_cov >= proj_cov:\n",
        "            risk = 100 + ( float(min_cov - proj_cov) / min_cov ) * 100\n",
        "            risk *= alpha_neg\n",
        "        else:\n",
        "            risk = 1 / ( (float(proj_cov - min_cov) / proj_cov) + 1)\n",
        "            risk *= alpha_pos\n",
        "        \n",
        "    # output risks #\n",
        "    return risks\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    We have reached out to Envrironmental Science professors to try and come up\n",
        "    with a formula to estimate the minimum viable forest cover that defines the min \n",
        "    forest cover for which most forest (and later other) ecosystems can fully rebound \n",
        "    from without additional human aid.\n",
        "\n",
        "    In other words, this is the minimum area needed for nature to solve the \n",
        "    deforestation problem on its own. Past this point, serious effort will be needed.\n",
        "\n",
        "    This formula is yet to be finalized, so for now we are making the assumption that \n",
        "    for any given forest data, its minimum point has thus far been recoverable (unless \n",
        "    the minimum point is the current point). Therefore, we will use the minimum recorded \n",
        "    data point until a better formula can be agreed upon.\n",
        "\"\"\"\n",
        "def calc_min_viable_cover(forest_data):\n",
        "    # variable defining #\n",
        "    min_historical_cov = np.min(forest_data)\n",
        "\n",
        "    # formula (yet to be finalized) #\n",
        "    min_viable_cov = min_historical_cov\n",
        "\n",
        "    return min_viable_cov\n",
        "\n"
      ],
      "metadata": {
        "id": "NHJa7FNyTaC8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}